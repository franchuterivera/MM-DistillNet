[DEFAULT]
# Name of the experiment to run. All data would be stored
# in a directory with this name, and the best checkpoint 
# Stored there also.
exp_name = FreiburgMultimodalDetection_YetAnotherEfficientDet_D2_multiteacher
log_path = tensorboard
saved_path = "trained_models"
fast_run = True

# ================Input configuration====================
# Dataset
dataset=FreiburgMultimodalDetection
data_path=/export/rivera/data/FreiburgMultimodalDetection 
id_filter=None
#id_filter=.*drive_day_2020_04_21_15_58_22.*
drive_type=all

# Ignore sheep and bottle as target predictions,
# EfficientDet can produce it but Yolo cannot
valid_labels=car

# This decides if labels are taken from teacher or from the
# Dataset
use_labels=False

# Modalitis to be used on the teachers
use_thermal=True
use_depth=True
use_rgb=True
use_audio=False

# The student is always single modality
student_modality=audio

# Sizes of the images. EfficientDet expects 512 and Yolo 416
image_size = 768
thermal_size = 768
depth_size = 768
audio_size = 768

# Modalities to be used by the

# Normalizes images to 1
normalize=True

# Transformations to use on each batched modality
train_transformations=Normalizer,Resizer
val_transformations=Normalizer,Resizer

# ================Train configuration====================
# Make run reproducible. Lesser than zero ignore this setting
seed = -24

# Batch size. EfficientDet can at most handle 32 wheras Yolo 80
batch_size = 20

# Enable Parallel Execution 
ngpu = 2
num_workers=10
#engine=DataParallel
engine=DataParallel
#engine=DistributedDataParallel
data_augment_shift=False

# Models. If _kaist is used, a pretrained version on Kaist 
# is used. That is, because original Kaist resolution is very 
# small a pretrain is needed
teacher=YetAnotherEfficientDet_D2
student=YetAnotherEfficientDet_D2
features_from=efficientnet

# Losses. Main loss provides classification and regression loss
# Divergence loss enables a KL penalty on the model outputs
# KD is the knowledge Distillation Loss at feature level
main_loss=YetAnotherFocalLoss
div_loss=None
kd_loss=HellingerLoss
adv_loss=None

T=4
p=2

# Weights to penalize more certain losses
w_main=1.0
w_div=1.0
w_kd=0.005
w_adv=1.0

# Training Information
resume = True
train_method=traditional_nms
integration_mode='concat'
es_patience = 10
num_epoches = 20
val_interval = 5

# BOHB information
enable_bohb = False
bohb_iterations = 4
enable_prev_bohb_run=False

#Adversarial Training Tweaks
pretrain=False
weights_init=False

#Optimizer
grad_clip = -1
optimizer=Adam
lr = 1e-4
momentum=0.9
weight_decay=5e-4
b1=0.9
b2=0.999

# Scheduler
scheduler=ReduceLROnPlateau
step_size = 3
gamma=0.1

# Evaluation information
iou_thres = 0.5
conf_threshold = 0.3
nms_threshold = 0.5
